### NLP 自然语言处理

就是让计算机读懂人们所写的文字，然后像人一样进行交互、聊天。



#### 图灵测试

判断一台计算机是不是机器人，是否具有人工智能

最初叫做Imitation Game，让机器去模仿一个人的表现



### NLP中的基础任务（待完善）

1. 标注
   1. Part of speech tagging 词性标注：标注出一个词是名词、动词、形容词等
   2. Named entity recognition 命名实体标注：主要是现实中的一些实体，比如人名、地名、日期等
   3. Co-reference 共指消解：指代词代表前面的哪个实体
   4. 词义角色标注：主谓宾
2. 分词（中文因为不带空格，需要分词）



### 一、词表示

NLP中最基本的语言单位：词。

把词转换，让计算机理解这个词的意思。

#### 目的：

##### 1. 词相似度计算
##### 2. 词之间的关系



#### 方法一。同义词和上位词

用一些相关的词来表示一个词

用同义词和上位词有些问题：

1. 有的词之间有一些细微的差异
2. 有的词会有新的定义（Apple，水果、IT公司），需要人工标注
3. 主观性
4. 数据吸收
5. 需要大量人工维护词典





#### 方法二。One-Hot Representation

*对于计算机来说，把一个词表示成一个独立的符号*

向量：一个字典中，有一个向量数组，每个词在相应的位置值为1，其他为0。这样每个词就可以有一个唯一的向量表示。

![image-20230807174204000](/images/image-20230807174204000.png)

问题：两个词之间的相似性没办法表示。

`silimarity(star, sun)=(Vstar, Vsun)=0`



#### 方法三。上下文表示法
一个词在句子中，有上下文，通过对一个词上下文出现的次数来表示

![image-20230807180001479](/images/image-20230807180001479.png)

问题：

1. 存储空间越来越大
2. 词出现的少，上下文就少，词的表示，效果就没那么好



#### 方法四。Word Embedding

深度学习这一派系的做法

用分布式的表示。

概念上而言，它是指把一个维数为所有词的数量的高维空间嵌入到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量。

1. Word2Vec





### 二、语言模型

一个任务，根据前面的词预测下一个词是什么

两个工作：

1. Joint probability：计算一个序列的词，它成为一句话的概率是多少
2. Conditional probability：一个词成为这句话下一个词的概率是多少



#### N-gram Model

该模型基于这样一种假设，第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。

常用的是二元的Bi-Gram和三元的Tri-Gram。

问题：一般都是二元或者三元，如果再多，存储以及概率计算效果不好；类似于One-Hot，词与词之间的相似度没办法计算。



#### Neural Language Model

针对N元模型的问题，提出了神经网络语言模型。

论文地址：[A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)

![image-20230808102406667](/images/image-20230808102406667.png)


